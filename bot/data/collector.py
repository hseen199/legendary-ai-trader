"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - Data Collector
Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Hugging Face Ùˆ Binance API
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import time
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from loguru import logger
from datasets import load_dataset
from tqdm import tqdm


class DataCollector:
    """
    Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©
    - Hugging Face (duonlabs/apogee)
    - Binance Public API
    """
    
    # Ø£ÙØ¶Ù„ 100 Ø¹Ù…Ù„Ø© Ù„Ù„ØªØ¯Ø§ÙˆÙ„
    TOP_100_SYMBOLS = [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "ADAUSDT",
        "DOGEUSDT", "SOLUSDT", "DOTUSDT", "MATICUSDT", "LTCUSDT",
        "SHIBUSDT", "TRXUSDT", "AVAXUSDT", "LINKUSDT", "ATOMUSDT",
        "UNIUSDT", "ETCUSDT", "XLMUSDT", "BCHUSDT", "FILUSDT",
        "LDOUSDT", "APTUSDT", "ARBUSDT", "OPUSDT", "NEARUSDT",
        "ICPUSDT", "VETUSDT", "HBARUSDT", "QNTUSDT", "AAVEUSDT",
        "GRTUSDT", "ALGOUSDT", "FTMUSDT", "SANDUSDT", "MANAUSDT",
        "AXSUSDT", "THETAUSDT", "EGLDUSDT", "EOSUSDT", "XTZUSDT",
        "FLOWUSDT", "CHZUSDT", "MKRUSDT", "SNXUSDT", "NEOUSDT",
        "RNDRUSDT", "KAVAUSDT", "MINAUSDT", "XMRUSDT", "BTCUSDT",
        "RUNEUSDT", "ZILUSDT", "ENJUSDT", "BATUSDT", "CRVUSDT",
        "LRCUSDT", "COMPUSDT", "YFIUSDT", "1INCHUSDT", "ANKRUSDT",
        "KSMUSDT", "DASHUSDT", "ZECUSDT", "WAVESUSDT", "IOSTUSDT",
        "ONTUSDT", "HOTUSDT", "ZENUSDT", "COTIUSDT", "SCUSDT",
        "DGBUSDT", "ICXUSDT", "RVNUSDT", "STXUSDT", "IOTAUSDT",
        "CELRUSDT", "CKBUSDT", "SXPUSDT", "RENUSDT", "OCEANUSDT",
        "RSRUSDT", "BLZUSDT", "CVCUSDT", "STMXUSDT", "DUSKUSDT",
        "ARUSDT", "CTSIUSDT", "MTLUSDT", "OGNUSDT", "NKNUSDT",
        "REEFUSDT", "LITUSDT", "SFPUSDT", "TLMUSDT", "ALICEUSDT",
        "LINAUSDT", "PERPUSDT", "RAREUSDT", "HIGHUSDT", "WLDUSDT"
    ]
    
    BINANCE_BASE_URL = "https://api.binance.com"
    
    def __init__(self, data_dir: str = "data/raw"):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            data_dir: Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“Š DataCollector initialized. Data dir: {self.data_dir}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HUGGING FACE DATA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def load_from_huggingface(
        self, 
        symbols: Optional[List[str]] = None,
        max_symbols: int = 100
    ) -> Dict[str, pd.DataFrame]:
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Hugging Face
        
        Args:
            symbols: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            max_symbols: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø¹Ù…Ù„Ø§Øª
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø¹Ù…Ù„Ø©
        """
        logger.info("ğŸ“¥ Loading data from Hugging Face (duonlabs/apogee)...")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            dataset = load_dataset("duonlabs/apogee", "binance", split="train")
            logger.info(f"âœ… Dataset loaded: {len(dataset)} records")
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrame
            df = dataset.to_pandas()
            logger.info(f"ğŸ“Š Columns: {df.columns.tolist()}")
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
            if 'symbol' in df.columns:
                available_symbols = df['symbol'].unique().tolist()
            else:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù…Ø² Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ø¹Ù…ÙˆØ¯ Ø¢Ø®Ø±
                available_symbols = self.TOP_100_SYMBOLS[:max_symbols]
            
            logger.info(f"ğŸ“Š Available symbols: {len(available_symbols)}")
            
            # ØªØµÙÙŠØ© Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            if symbols:
                target_symbols = [s for s in symbols if s in available_symbols]
            else:
                target_symbols = available_symbols[:max_symbols]
            
            # ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ù„Ø©
            data_dict = {}
            
            if 'symbol' in df.columns:
                for symbol in tqdm(target_symbols, desc="Processing symbols"):
                    symbol_df = df[df['symbol'] == symbol].copy()
                    if len(symbol_df) > 0:
                        symbol_df = self._standardize_columns(symbol_df)
                        data_dict[symbol] = symbol_df
            else:
                # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø®ØªÙ„Ù
                df = self._standardize_columns(df)
                data_dict['COMBINED'] = df
            
            logger.info(f"âœ… Loaded data for {len(data_dict)} symbols")
            return data_dict
            
        except Exception as e:
            logger.error(f"âŒ Error loading from HuggingFace: {e}")
            logger.info("ğŸ“¥ Falling back to Binance API...")
            return self.fetch_from_binance(symbols or self.TOP_100_SYMBOLS[:max_symbols])
    
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©"""
        column_mapping = {
            'Open': 'open', 'High': 'high', 'Low': 'low', 
            'Close': 'close', 'Volume': 'volume',
            'open_time': 'timestamp', 'close_time': 'close_timestamp',
            'Timestamp': 'timestamp', 'Date': 'timestamp',
            'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'
        }
        
        df = df.rename(columns=column_mapping)
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        required = ['open', 'high', 'low', 'close', 'volume']
        for col in required:
            if col not in df.columns:
                logger.warning(f"âš ï¸ Missing column: {col}")
        
        # ØªØ­ÙˆÙŠÙ„ timestamp Ø¥Ù„Ù‰ datetime
        if 'timestamp' in df.columns:
            if df['timestamp'].dtype in ['int64', 'float64']:
                # ØªØ­ÙˆÙŠÙ„ Ù…Ù† milliseconds
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            else:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            df = df.set_index('timestamp').sort_index()
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        for col in required:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BINANCE API DATA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def fetch_from_binance(
        self,
        symbols: List[str],
        interval: str = "1h",
        days: int = 180
    ) -> Dict[str, pd.DataFrame]:
        """
        Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Binance API
        
        Args:
            symbols: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª
            interval: Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ
            days: Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù…
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø¹Ù…Ù„Ø©
        """
        logger.info(f"ğŸ“¥ Fetching data from Binance API for {len(symbols)} symbols...")
        
        data_dict = {}
        
        for symbol in tqdm(symbols, desc="Fetching from Binance"):
            try:
                df = self._fetch_klines(symbol, interval, days)
                if df is not None and len(df) > 0:
                    data_dict[symbol] = df
                    time.sleep(0.1)  # ØªØ¬Ù†Ø¨ ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to fetch {symbol}: {e}")
                continue
        
        logger.info(f"âœ… Fetched data for {len(data_dict)} symbols")
        return data_dict
    
    def _fetch_klines(
        self, 
        symbol: str, 
        interval: str = "1h",
        days: int = 180
    ) -> Optional[pd.DataFrame]:
        """
        Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ù…ÙˆØ¹ Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©
        
        Args:
            symbol: Ø±Ù…Ø² Ø§Ù„Ø¹Ù…Ù„Ø©
            interval: Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ
            days: Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù…
            
        Returns:
            DataFrame Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        endpoint = f"{self.BINANCE_BASE_URL}/api/v3/klines"
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª
        end_time = int(datetime.now().timestamp() * 1000)
        start_time = int((datetime.now() - timedelta(days=days)).timestamp() * 1000)
        
        all_data = []
        current_start = start_time
        
        while current_start < end_time:
            params = {
                "symbol": symbol,
                "interval": interval,
                "startTime": current_start,
                "endTime": end_time,
                "limit": 1000
            }
            
            try:
                response = requests.get(endpoint, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                if not data:
                    break
                
                all_data.extend(data)
                current_start = data[-1][0] + 1
                
            except Exception as e:
                logger.warning(f"âš ï¸ Error fetching {symbol}: {e}")
                break
        
        if not all_data:
            return None
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrame
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df = df.set_index('timestamp')
        
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙ‚Ø·
        df = df[['open', 'high', 'low', 'close', 'volume']]
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MULTI-TIMEFRAME DATA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def fetch_multi_timeframe(
        self,
        symbols: List[str],
        timeframes: List[str] = ["1m", "5m", "15m", "1h", "4h"],
        days: int = 30
    ) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        
        Args:
            symbols: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª
            timeframes: Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            days: Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù…
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ù…ØªØ¯Ø§Ø®Ù„ {symbol: {timeframe: DataFrame}}
        """
        logger.info(f"ğŸ“¥ Fetching multi-timeframe data for {len(symbols)} symbols...")
        
        result = {}
        
        for symbol in tqdm(symbols, desc="Fetching symbols"):
            result[symbol] = {}
            
            for tf in timeframes:
                try:
                    df = self._fetch_klines(symbol, tf, days)
                    if df is not None:
                        result[symbol][tf] = df
                    time.sleep(0.05)
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed {symbol} {tf}: {e}")
        
        return result
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DATA SAVING & LOADING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def save_data(
        self, 
        data_dict: Dict[str, pd.DataFrame],
        prefix: str = "ohlcv"
    ) -> None:
        """
        Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª
        
        Args:
            data_dict: Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            prefix: Ø¨Ø§Ø¯Ø¦Ø© Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        """
        save_dir = self.data_dir / prefix
        save_dir.mkdir(parents=True, exist_ok=True)
        
        for symbol, df in data_dict.items():
            filepath = save_dir / f"{symbol}.parquet"
            df.to_parquet(filepath)
        
        logger.info(f"âœ… Saved {len(data_dict)} files to {save_dir}")
    
    def load_data(
        self, 
        symbols: Optional[List[str]] = None,
        prefix: str = "ohlcv"
    ) -> Dict[str, pd.DataFrame]:
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
        
        Args:
            symbols: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            prefix: Ø¨Ø§Ø¯Ø¦Ø© Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        load_dir = self.data_dir / prefix
        
        if not load_dir.exists():
            logger.warning(f"âš ï¸ Directory not found: {load_dir}")
            return {}
        
        data_dict = {}
        files = list(load_dir.glob("*.parquet"))
        
        for filepath in files:
            symbol = filepath.stem
            if symbols is None or symbol in symbols:
                df = pd.read_parquet(filepath)
                data_dict[symbol] = df
        
        logger.info(f"âœ… Loaded {len(data_dict)} files from {load_dir}")
        return data_dict
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TOP SYMBOLS BY VOLUME
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_top_symbols_by_volume(
        self, 
        limit: int = 100,
        quote_asset: str = "USDT"
    ) -> List[str]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…
        
        Args:
            limit: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„Ø§Øª
            quote_asset: Ø¹Ù…Ù„Ø© Ø§Ù„ØªØ³Ø¹ÙŠØ±
            
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ù…Ù„Ø§Øª
        """
        endpoint = f"{self.BINANCE_BASE_URL}/api/v3/ticker/24hr"
        
        try:
            response = requests.get(endpoint, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            # ØªØµÙÙŠØ© Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø­Ø³Ø¨ Ø¹Ù…Ù„Ø© Ø§Ù„ØªØ³Ø¹ÙŠØ±
            filtered = [
                d for d in data 
                if d['symbol'].endswith(quote_asset)
            ]
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…
            sorted_data = sorted(
                filtered, 
                key=lambda x: float(x['quoteVolume']), 
                reverse=True
            )
            
            symbols = [d['symbol'] for d in sorted_data[:limit]]
            logger.info(f"âœ… Found top {len(symbols)} symbols by volume")
            
            return symbols
            
        except Exception as e:
            logger.error(f"âŒ Error getting top symbols: {e}")
            return self.TOP_100_SYMBOLS[:limit]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DATA VALIDATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def validate_data(self, df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            df: DataFrame Ù„Ù„ØªØ­Ù‚Ù‚
            
        Returns:
            (ØµØ§Ù„Ø­, Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
        """
        errors = []
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        required = ['open', 'high', 'low', 'close', 'volume']
        missing = [c for c in required if c not in df.columns]
        if missing:
            errors.append(f"Missing columns: {missing}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ…
        if (df['high'] < df['low']).any():
            errors.append("High < Low detected")
        
        if (df['volume'] < 0).any():
            errors.append("Negative volume detected")
        
        if df.isnull().any().any():
            null_counts = df.isnull().sum()
            errors.append(f"Null values: {null_counts[null_counts > 0].to_dict()}")
        
        return len(errors) == 0, errors


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆØ­Ø¯Ø©
    collector = DataCollector()
    
    # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    data = collector.load_from_huggingface(max_symbols=10)
    
    if data:
        for symbol, df in list(data.items())[:3]:
            print(f"\n{symbol}:")
            print(f"  Shape: {df.shape}")
            print(f"  Columns: {df.columns.tolist()}")
            print(f"  Date range: {df.index.min()} to {df.index.max()}")
