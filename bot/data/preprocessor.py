"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - Data Preprocessor
Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from loguru import logger
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import warnings
warnings.filterwarnings('ignore')


class DataPreprocessor:
    """
    Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
    - ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
    """
    
    def __init__(self, scaling_method: str = "robust"):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        
        Args:
            scaling_method: Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (standard, minmax, robust)
        """
        self.scaling_method = scaling_method
        self.scalers: Dict[str, Any] = {}
        self.feature_stats: Dict[str, Dict] = {}
        
        logger.info(f"ðŸ”§ DataPreprocessor initialized with {scaling_method} scaling")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MAIN PROCESSING PIPELINE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def process(
        self, 
        df: pd.DataFrame,
        symbol: str = "UNKNOWN",
        remove_outliers: bool = True,
        fill_missing: bool = True,
        scale: bool = True
    ) -> pd.DataFrame:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        
        Args:
            df: DataFrame Ø§Ù„Ø®Ø§Ù…
            symbol: Ø±Ù…Ø² Ø§Ù„Ø¹Ù…Ù„Ø©
            remove_outliers: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
            fill_missing: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            scale: ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            
        Returns:
            DataFrame Ù…Ø¹Ø§Ù„Ø¬
        """
        logger.info(f"ðŸ”§ Processing {symbol}: {len(df)} rows")
        
        # Ù†Ø³Ø®Ø© Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡Ø§
        df = df.copy()
        
        # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£ÙˆÙ„ÙŠ
        df = self._validate_and_clean(df)
        
        # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        if fill_missing:
            df = self._fill_missing_values(df)
        
        # 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
        if remove_outliers:
            df = self._remove_outliers(df)
        
        # 4. Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        if scale:
            df = self._scale_data(df, symbol)
        
        # 5. Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self._save_stats(df, symbol)
        
        logger.info(f"âœ… Processed {symbol}: {len(df)} rows remaining")
        return df
    
    def process_batch(
        self, 
        data_dict: Dict[str, pd.DataFrame],
        **kwargs
    ) -> Dict[str, pd.DataFrame]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            data_dict: Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        processed = {}
        
        for symbol, df in data_dict.items():
            try:
                processed[symbol] = self.process(df, symbol, **kwargs)
            except Exception as e:
                logger.error(f"âŒ Error processing {symbol}: {e}")
                continue
        
        return processed
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION & CLEANING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _validate_and_clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø§Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ÙŠ"""
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        required = ['open', 'high', 'low', 'close', 'volume']
        for col in required:
            if col not in df.columns:
                raise ValueError(f"Missing required column: {col}")
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
        for col in required:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ© ØªÙ…Ø§Ù…Ø§Ù‹
        df = df.dropna(how='all')
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©
        # high ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† >= open, close, low
        df['high'] = df[['open', 'high', 'low', 'close']].max(axis=1)
        # low ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† <= open, close, high
        df['low'] = df[['open', 'high', 'low', 'close']].min(axis=1)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ù„Ø¨Ø©
        for col in required:
            df = df[df[col] >= 0]
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        df = df[~df.index.duplicated(keep='first')]
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª
        df = df.sort_index()
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MISSING VALUES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _fill_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©"""
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        missing_pct = df.isnull().sum() / len(df) * 100
        
        for col in df.columns:
            if df[col].isnull().any():
                if missing_pct[col] > 50:
                    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙˆØ³ÙŠØ·
                    df[col] = df[col].fillna(df[col].median())
                else:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³ØªÙŠÙØ§Ø¡ Ø§Ù„Ø®Ø·ÙŠ
                    df[col] = df[col].interpolate(method='linear')
                    # Ù…Ù„Ø¡ Ø£ÙŠ Ù‚ÙŠÙ… Ù…ØªØ¨Ù‚ÙŠØ© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©/Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # OUTLIER REMOVAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _remove_outliers(
        self, 
        df: pd.DataFrame,
        method: str = "iqr",
        threshold: float = 3.0
    ) -> pd.DataFrame:
        """
        Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
        
        Args:
            df: DataFrame
            method: Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙƒØ´Ù (iqr, zscore)
            threshold: Ø¹ØªØ¨Ø© Ø§Ù„ÙƒØ´Ù
        """
        original_len = len(df)
        
        if method == "iqr":
            df = self._remove_outliers_iqr(df, threshold)
        elif method == "zscore":
            df = self._remove_outliers_zscore(df, threshold)
        
        removed = original_len - len(df)
        if removed > 0:
            logger.debug(f"  Removed {removed} outliers ({removed/original_len*100:.2f}%)")
        
        return df
    
    def _remove_outliers_iqr(self, df: pd.DataFrame, multiplier: float = 1.5) -> pd.DataFrame:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… IQR"""
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© OHLCV ÙÙ‚Ø·
        columns = ['open', 'high', 'low', 'close', 'volume']
        
        for col in columns:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower = Q1 - multiplier * IQR
                upper = Q3 + multiplier * IQR
                
                # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¥Ø²Ø§Ù„Ø©ØŒ Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„Ù‚Øµ
                df[col] = df[col].clip(lower=lower, upper=upper)
        
        return df
    
    def _remove_outliers_zscore(self, df: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Z-score"""
        
        columns = ['open', 'high', 'low', 'close', 'volume']
        
        for col in columns:
            if col in df.columns:
                mean = df[col].mean()
                std = df[col].std()
                
                if std > 0:
                    z_scores = np.abs((df[col] - mean) / std)
                    df = df[z_scores < threshold]
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SCALING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _scale_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙØ·Ø¨Ù‘Ø¹
        if self.scaling_method == "standard":
            scaler = StandardScaler()
        elif self.scaling_method == "minmax":
            scaler = MinMaxScaler()
        else:
            scaler = RobustScaler()
        
        # ØªØ·Ø¨ÙŠØ¹ Ø£Ø¹Ù…Ø¯Ø© OHLCV
        columns_to_scale = ['open', 'high', 'low', 'close', 'volume']
        columns_present = [c for c in columns_to_scale if c in df.columns]
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„Ø¹ÙƒØ³ Ù„Ø§Ø­Ù‚Ø§Ù‹
        self.scalers[symbol] = {
            'scaler': scaler,
            'columns': columns_present,
            'original_values': {
                col: {'min': df[col].min(), 'max': df[col].max()}
                for col in columns_present
            }
        }
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        df[columns_present] = scaler.fit_transform(df[columns_present])
        
        return df
    
    def inverse_scale(
        self, 
        df: pd.DataFrame, 
        symbol: str
    ) -> pd.DataFrame:
        """Ø¹ÙƒØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹"""
        
        if symbol not in self.scalers:
            logger.warning(f"âš ï¸ No scaler found for {symbol}")
            return df
        
        scaler_info = self.scalers[symbol]
        columns = scaler_info['columns']
        scaler = scaler_info['scaler']
        
        df[columns] = scaler.inverse_transform(df[columns])
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STATISTICS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _save_stats(self, df: pd.DataFrame, symbol: str) -> None:
        """Ø­ÙØ¸ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        
        self.feature_stats[symbol] = {
            'count': len(df),
            'date_range': {
                'start': str(df.index.min()),
                'end': str(df.index.max())
            },
            'columns': {}
        }
        
        for col in df.columns:
            self.feature_stats[symbol]['columns'][col] = {
                'mean': float(df[col].mean()),
                'std': float(df[col].std()),
                'min': float(df[col].min()),
                'max': float(df[col].max()),
                'median': float(df[col].median())
            }
    
    def get_stats(self, symbol: str = None) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        if symbol:
            return self.feature_stats.get(symbol, {})
        return self.feature_stats
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SPECIAL TRANSFORMATIONS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def add_returns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯"""
        df = df.copy()
        
        # Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
        df['returns'] = df['close'].pct_change()
        
        # Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ù„ÙˆØºØ§Ø±ÙŠØªÙ…ÙŠØ©
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        
        # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£ÙˆÙ„Ù‰
        df['returns'] = df['returns'].fillna(0)
        df['log_returns'] = df['log_returns'].fillna(0)
        
        return df
    
    def add_price_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø±"""
        df = df.copy()
        
        # Ù†Ø·Ø§Ù‚ Ø§Ù„Ø´Ù…Ø¹Ø©
        df['candle_range'] = df['high'] - df['low']
        df['candle_body'] = abs(df['close'] - df['open'])
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø¬Ø³Ù… Ù„Ù„Ù†Ø·Ø§Ù‚
        df['body_ratio'] = df['candle_body'] / (df['candle_range'] + 1e-10)
        
        # Ø§Ù„Ø¸Ù„ Ø§Ù„Ø¹Ù„ÙˆÙŠ ÙˆØ§Ù„Ø³ÙÙ„ÙŠ
        df['upper_shadow'] = df['high'] - df[['open', 'close']].max(axis=1)
        df['lower_shadow'] = df[['open', 'close']].min(axis=1) - df['low']
        
        # Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø´Ù…Ø¹Ø©
        df['candle_direction'] = np.where(df['close'] >= df['open'], 1, -1)
        
        return df
    
    def add_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø¬Ù…"""
        df = df.copy()
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ø¬Ù…
        df['volume_ma_20'] = df['volume'].rolling(window=20).mean()
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø­Ø¬Ù… Ù„Ù„Ù…ØªÙˆØ³Ø·
        df['volume_ratio'] = df['volume'] / (df['volume_ma_20'] + 1e-10)
        
        # ØªØºÙŠØ± Ø§Ù„Ø­Ø¬Ù…
        df['volume_change'] = df['volume'].pct_change()
        
        # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        df = df.fillna(method='bfill').fillna(0)
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SEQUENCE CREATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def create_sequences(
        self,
        df: pd.DataFrame,
        sequence_length: int = 60,
        target_column: str = 'close',
        prediction_horizon: int = 1
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ù„Ø³Ù„Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        
        Args:
            df: DataFrame
            sequence_length: Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            target_column: Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù
            prediction_horizon: Ø£ÙÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤
            
        Returns:
            (X, y) Ù…ØµÙÙˆÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        """
        data = df.values
        target_idx = df.columns.get_loc(target_column)
        
        X, y = [], []
        
        for i in range(len(data) - sequence_length - prediction_horizon + 1):
            X.append(data[i:i+sequence_length])
            
            # Ø§Ù„Ù‡Ø¯Ù: ØªØºÙŠØ± Ø§Ù„Ø³Ø¹Ø±
            current_price = data[i+sequence_length-1, target_idx]
            future_price = data[i+sequence_length+prediction_horizon-1, target_idx]
            
            if current_price != 0:
                change = (future_price - current_price) / current_price
            else:
                change = 0
            
            y.append(change)
        
        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)
    
    def create_classification_labels(
        self,
        y: np.ndarray,
        threshold: float = 0.01
    ) -> np.ndarray:
        """
        ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª
        
        Args:
            y: Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØºÙŠØ±Ø§Øª
            threshold: Ø¹ØªØ¨Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
            
        Returns:
            Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª (0=BUY, 1=SELL, 2=HOLD)
        """
        labels = np.zeros(len(y), dtype=np.int64)
        
        labels[y > threshold] = 0   # BUY
        labels[y < -threshold] = 1  # SELL
        labels[(y >= -threshold) & (y <= threshold)] = 2  # HOLD
        
        return labels


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆØ­Ø¯Ø©
    preprocessor = DataPreprocessor()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    dates = pd.date_range(start='2024-01-01', periods=1000, freq='1H')
    df = pd.DataFrame({
        'open': np.random.uniform(40000, 50000, 1000),
        'high': np.random.uniform(40000, 51000, 1000),
        'low': np.random.uniform(39000, 50000, 1000),
        'close': np.random.uniform(40000, 50000, 1000),
        'volume': np.random.uniform(100, 10000, 1000)
    }, index=dates)
    
    # Ø¥ØµÙ„Ø§Ø­ high/low
    df['high'] = df[['open', 'high', 'close']].max(axis=1)
    df['low'] = df[['open', 'low', 'close']].min(axis=1)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø©
    processed = preprocessor.process(df, "BTCUSDT")
    
    print(f"Original shape: {df.shape}")
    print(f"Processed shape: {processed.shape}")
    print(f"\nStats: {preprocessor.get_stats('BTCUSDT')}")
