"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - Base Model
Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import torch
import torch.nn as nn
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from pathlib import Path
from loguru import logger


class BaseModel(ABC, nn.Module):
    """
    Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
    ÙŠÙˆÙØ± ÙˆØ§Ø¬Ù‡Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ø­ÙØ¸
    """
    
    def __init__(self, name: str, config: Dict[str, Any]):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        
        Args:
            name: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        super().__init__()
        self.name = name
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.is_trained = False
        self.training_history: List[Dict] = []
        self.best_loss = float('inf')
        
        logger.info(f"ğŸ§  {name} initialized on {self.device}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ABSTRACT METHODS - ÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ°Ù‡Ø§ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ"""
        pass
    
    @abstractmethod
    def predict(self, x: np.ndarray) -> Dict[str, Any]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤"""
        pass
    
    @abstractmethod
    def get_input_shape(self) -> Tuple[int, ...]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª"""
        pass
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TRAINING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def train_model(
        self,
        train_loader: torch.utils.data.DataLoader,
        val_loader: Optional[torch.utils.data.DataLoader] = None,
        epochs: int = 100,
        learning_rate: float = 0.001,
        early_stopping_patience: int = 10,
        save_best: bool = True,
        save_dir: str = "checkpoints"
    ) -> Dict[str, List[float]]:
        """
        ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            train_loader: Ù…Ø­Ù…Ù‘Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            val_loader: Ù…Ø­Ù…Ù‘Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚
            epochs: Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨
            learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            early_stopping_patience: ØµØ¨Ø± Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø¨ÙƒØ±
            save_best: Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            save_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
            
        Returns:
            ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        """
        self.to(self.device)
        self.train()
        
        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )
        criterion = self._get_loss_function()
        
        history = {'train_loss': [], 'val_loss': []}
        best_val_loss = float('inf')
        patience_counter = 0
        
        logger.info(f"ğŸš€ Starting training for {epochs} epochs...")
        
        for epoch in range(epochs):
            # Training
            train_loss = self._train_epoch(train_loader, optimizer, criterion)
            history['train_loss'].append(train_loss)
            
            # Validation
            if val_loader:
                val_loss = self._validate_epoch(val_loader, criterion)
                history['val_loss'].append(val_loss)
                scheduler.step(val_loss)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    if save_best:
                        self._save_checkpoint(save_dir, "best")
                else:
                    patience_counter += 1
                    if patience_counter >= early_stopping_patience:
                        logger.info(f"â¹ï¸ Early stopping at epoch {epoch+1}")
                        break
                
                logger.info(
                    f"Epoch {epoch+1}/{epochs} | "
                    f"Train Loss: {train_loss:.6f} | "
                    f"Val Loss: {val_loss:.6f}"
                )
            else:
                logger.info(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.6f}")
        
        self.is_trained = True
        self.training_history = history
        self.best_loss = best_val_loss if val_loader else train_loss
        
        logger.info(f"âœ… Training completed. Best loss: {self.best_loss:.6f}")
        return history
    
    def _train_epoch(
        self,
        loader: torch.utils.data.DataLoader,
        optimizer: torch.optim.Optimizer,
        criterion: nn.Module
    ) -> float:
        """ØªØ¯Ø±ÙŠØ¨ Ø­Ù‚Ø¨Ø© ÙˆØ§Ø­Ø¯Ø©"""
        self.train()
        total_loss = 0.0
        num_batches = 0
        
        for batch_x, batch_y in loader:
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.forward(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        return total_loss / num_batches
    
    def _validate_epoch(
        self,
        loader: torch.utils.data.DataLoader,
        criterion: nn.Module
    ) -> float:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ù‚Ø¨Ø© ÙˆØ§Ø­Ø¯Ø©"""
        self.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch_x, batch_y in loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                outputs = self.forward(batch_x)
                loss = criterion(outputs, batch_y)
                
                total_loss += loss.item()
                num_batches += 1
        
        return total_loss / num_batches
    
    def _get_loss_function(self) -> nn.Module:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©"""
        return nn.MSELoss()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SAVE & LOAD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _save_checkpoint(self, save_dir: str, tag: str = "latest") -> str:
        """Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´"""
        os.makedirs(save_dir, exist_ok=True)
        filepath = os.path.join(save_dir, f"{self.name}_{tag}.pt")
        
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'config': self.config,
            'is_trained': self.is_trained,
            'best_loss': self.best_loss,
            'training_history': self.training_history,
            'timestamp': datetime.now().isoformat()
        }
        
        torch.save(checkpoint, filepath)
        logger.debug(f"ğŸ’¾ Checkpoint saved: {filepath}")
        return filepath
    
    def save(self, filepath: str) -> None:
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'config': self.config,
            'is_trained': self.is_trained,
            'best_loss': self.best_loss,
            'training_history': self.training_history,
            'timestamp': datetime.now().isoformat()
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        torch.save(checkpoint, filepath)
        logger.info(f"ğŸ’¾ Model saved: {filepath}")
    
    def load(self, filepath: str) -> None:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
        
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.load_state_dict(checkpoint['model_state_dict'])
        self.is_trained = checkpoint.get('is_trained', True)
        self.best_loss = checkpoint.get('best_loss', float('inf'))
        self.training_history = checkpoint.get('training_history', [])
        
        logger.info(f"ğŸ“‚ Model loaded: {filepath}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ONNX EXPORT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def export_onnx(
        self,
        filepath: str,
        input_shape: Optional[Tuple[int, ...]] = None,
        opset_version: int = 14
    ) -> str:
        """
        ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ONNX
        
        Args:
            filepath: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
            input_shape: Ø´ÙƒÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            opset_version: Ø¥ØµØ¯Ø§Ø± ONNX
            
        Returns:
            Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙØµØ¯Ù‘Ø±
        """
        self.eval()
        self.to("cpu")
        
        if input_shape is None:
            input_shape = self.get_input_shape()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø®Ù„Ø§Øª ÙˆÙ‡Ù…ÙŠØ©
        dummy_input = torch.randn(1, *input_shape)
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        torch.onnx.export(
            self,
            dummy_input,
            filepath,
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        logger.info(f"ğŸ“¦ Model exported to ONNX: {filepath}")
        return filepath
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # UTILITIES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def count_parameters(self) -> int:
        """Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_model_info(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        return {
            'name': self.name,
            'parameters': self.count_parameters(),
            'is_trained': self.is_trained,
            'best_loss': self.best_loss,
            'device': str(self.device),
            'config': self.config
        }
    
    def to_device(self, device: str = None) -> 'BaseModel':
        """Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø¬Ù‡Ø§Ø²"""
        if device:
            self.device = torch.device(device)
        return self.to(self.device)


class TradingDataset(torch.utils.data.Dataset):
    """
    Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø§ÙˆÙ„
    """
    
    def __init__(
        self,
        X: np.ndarray,
        y: np.ndarray,
        transform: Optional[callable] = None
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            X: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            y: Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            transform: ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
        """
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y) if y.dtype != np.int64 else torch.LongTensor(y)
        self.transform = transform
    
    def __len__(self) -> int:
        return len(self.X)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        x = self.X[idx]
        y = self.y[idx]
        
        if self.transform:
            x = self.transform(x)
        
        return x, y


def create_data_loaders(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: Optional[np.ndarray] = None,
    y_val: Optional[np.ndarray] = None,
    batch_size: int = 64,
    shuffle: bool = True
) -> Tuple[torch.utils.data.DataLoader, Optional[torch.utils.data.DataLoader]]:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ù…Ù‘Ù„Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    
    Args:
        X_train: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        y_train: ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        X_val: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚
        y_val: ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ØªØ­Ù‚Ù‚
        batch_size: Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
        shuffle: Ø®Ù„Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
    Returns:
        (train_loader, val_loader)
    """
    train_dataset = TradingDataset(X_train, y_train)
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=0,
        pin_memory=True
    )
    
    val_loader = None
    if X_val is not None and y_val is not None:
        val_dataset = TradingDataset(X_val, y_val)
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True
        )
    
    return train_loader, val_loader
