"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - Temporal Fusion Transformer (TFT)
Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from loguru import logger

from .base_model import BaseModel


class GatedLinearUnit(nn.Module):
    """ÙˆØ­Ø¯Ø© Ø®Ø·ÙŠØ© Ù…Ø¨ÙˆØ¨Ø© (GLU)"""
    
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        self.fc = nn.Linear(input_dim, output_dim * 2)
        self.output_dim = output_dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.fc(x)
        return x[:, :, :self.output_dim] * torch.sigmoid(x[:, :, self.output_dim:])


class GatedResidualNetwork(nn.Module):
    """Ø´Ø¨ÙƒØ© Ø§Ù„Ø¨Ù‚Ø§ÙŠØ§ Ø§Ù„Ù…Ø¨ÙˆØ¨Ø© (GRN)"""
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        dropout: float = 0.1,
        context_dim: Optional[int] = None
    ):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.context_dim = context_dim
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.elu = nn.ELU()
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.glu = GatedLinearUnit(hidden_dim, output_dim)
        self.layer_norm = nn.LayerNorm(output_dim)
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©)
        if context_dim is not None:
            self.context_fc = nn.Linear(context_dim, hidden_dim, bias=False)
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ®Ø·ÙŠ
        if input_dim != output_dim:
            self.skip = nn.Linear(input_dim, output_dim)
        else:
            self.skip = None
    
    def forward(
        self, 
        x: torch.Tensor, 
        context: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        # Ø§Ù„ØªØ®Ø·ÙŠ
        if self.skip is not None:
            residual = self.skip(x)
        else:
            residual = x
        
        # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        x = self.fc1(x)
        
        if context is not None and self.context_dim is not None:
            context = self.context_fc(context)
            x = x + context.unsqueeze(1) if context.dim() == 2 else x + context
        
        x = self.elu(x)
        x = self.fc2(x)
        x = self.dropout(x)
        x = self.glu(x)
        
        # Ø§Ù„Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ø¨Ù‚Ø§ÙŠØ§
        return self.layer_norm(x + residual)


class VariableSelectionNetwork(nn.Module):
    """Ø´Ø¨ÙƒØ© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª"""
    
    def __init__(
        self,
        input_dim: int,
        num_inputs: int,
        hidden_dim: int,
        dropout: float = 0.1,
        context_dim: Optional[int] = None
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_inputs = num_inputs
        
        # GRN Ù„ÙƒÙ„ Ù…ØªØºÙŠØ±
        self.grns = nn.ModuleList([
            GatedResidualNetwork(input_dim, hidden_dim, hidden_dim, dropout)
            for _ in range(num_inputs)
        ])
        
        # GRN Ù„Ù„Ø£ÙˆØ²Ø§Ù†
        self.weight_grn = GatedResidualNetwork(
            hidden_dim * num_inputs,
            hidden_dim,
            num_inputs,
            dropout,
            context_dim
        )
        
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(
        self, 
        x: torch.Tensor,
        context: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…ØªØºÙŠØ±
        processed = []
        for i, grn in enumerate(self.grns):
            processed.append(grn(x[:, :, i:i+1].expand(-1, -1, self.hidden_dim)))
        
        processed = torch.stack(processed, dim=-1)  # [batch, seq, hidden, num_inputs]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        flat = processed.view(x.size(0), x.size(1), -1)
        weights = self.weight_grn(flat, context)
        weights = self.softmax(weights)
        
        # Ø§Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…ÙˆØ²ÙˆÙ†
        output = (processed * weights.unsqueeze(2)).sum(dim=-1)
        
        return output, weights


class InterpretableMultiHeadAttention(nn.Module):
    """Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙØ³ÙŠØ±"""
    
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"
        
        self.q_linear = nn.Linear(embed_dim, embed_dim)
        self.k_linear = nn.Linear(embed_dim, embed_dim)
        self.v_linear = nn.Linear(embed_dim, embed_dim)
        self.out_linear = nn.Linear(embed_dim, embed_dim)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = self.head_dim ** -0.5
    
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        
        # Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©
        q = self.q_linear(query)
        k = self.k_linear(key)
        v = self.v_linear(value)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„Ø±Ø¤ÙˆØ³ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        context = torch.matmul(attention_weights, v)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)
        output = self.out_linear(context)
        
        # Ù…ØªÙˆØ³Ø· Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¹Ø¨Ø± Ø§Ù„Ø±Ø¤ÙˆØ³
        avg_attention = attention_weights.mean(dim=1)
        
        return output, avg_attention


class TFTModel(BaseModel):
    """
    Ù†Ù…ÙˆØ°Ø¬ Temporal Fusion Transformer
    
    Ù…ØµÙ…Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù…Ø¹:
    - Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
    - Ø§Ù†ØªØ¨Ø§Ù‡ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙØ³ÙŠØ±
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØºÙŠØ±Ø§Øª Ø«Ø§Ø¨ØªØ© ÙˆÙ…ØªØºÙŠØ±Ø©
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        num_heads: int = 4,
        num_encoder_layers: int = 3,
        dropout: float = 0.1,
        sequence_length: int = 60,
        output_dim: int = 3,  # BUY, SELL, HOLD
        config: Optional[Dict] = None
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            input_dim: Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            hidden_dim: Ø­Ø¬Ù… Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
            num_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            num_encoder_layers: Ø¹Ø¯Ø¯ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø´ÙØ±
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·
            sequence_length: Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            output_dim: Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        """
        config = config or {}
        config.update({
            'input_dim': input_dim,
            'hidden_dim': hidden_dim,
            'num_heads': num_heads,
            'num_encoder_layers': num_encoder_layers,
            'dropout': dropout,
            'sequence_length': sequence_length,
            'output_dim': output_dim
        })
        
        super().__init__("TFT", config)
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_encoder_layers = num_encoder_layers
        self.sequence_length = sequence_length
        self.output_dim = output_dim
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        self.input_embedding = nn.Linear(input_dim, hidden_dim)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        self.variable_selection = VariableSelectionNetwork(
            input_dim=1,
            num_inputs=input_dim,
            hidden_dim=hidden_dim,
            dropout=dropout
        )
        
        # LSTM Ù„Ù„ØªØ´ÙÙŠØ±
        self.lstm_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=dropout if num_encoder_layers > 1 else 0,
            bidirectional=False
        )
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        self.attention_layers = nn.ModuleList([
            InterpretableMultiHeadAttention(hidden_dim, num_heads, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        # GRN Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        self.post_attention_grn = nn.ModuleList([
            GatedResidualNetwork(hidden_dim, hidden_dim, hidden_dim, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(hidden_dim)
            for _ in range(num_encoder_layers)
        ])
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
        
        logger.info(f"ğŸ§  TFT Model: {self.count_parameters():,} parameters")
    
    def _init_weights(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch, sequence, features]
            
        Returns:
            Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª [batch, output_dim]
        """
        batch_size = x.size(0)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        selected, var_weights = self.variable_selection(x)
        
        # ØªØ´ÙÙŠØ± LSTM
        lstm_out, (h_n, c_n) = self.lstm_encoder(selected)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attention_output = lstm_out
        attention_weights_list = []
        
        for i in range(self.num_encoder_layers):
            # Self-attention
            attn_out, attn_weights = self.attention_layers[i](
                attention_output, attention_output, attention_output
            )
            attention_weights_list.append(attn_weights)
            
            # Add & Norm
            attention_output = self.layer_norms[i](attention_output + attn_out)
            
            # GRN
            attention_output = self.post_attention_grn[i](attention_output)
        
        # Ø£Ø®Ø° Ø¢Ø®Ø± Ø®Ø·ÙˆØ© Ø²Ù…Ù†ÙŠØ©
        final_output = attention_output[:, -1, :]
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        output = self.output_layer(final_output)
        
        return output
    
    def predict(self, x: np.ndarray) -> Dict[str, Any]:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch, sequence, features] Ø£Ùˆ [sequence, features]
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        """
        self.eval()
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        if x.ndim == 2:
            x = x[np.newaxis, :, :]
        
        with torch.no_grad():
            x_tensor = torch.FloatTensor(x).to(self.device)
            logits = self.forward(x_tensor)
            probs = F.softmax(logits, dim=-1)
            predictions = torch.argmax(probs, dim=-1)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy
        probs_np = probs.cpu().numpy()
        preds_np = predictions.cpu().numpy()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡
        action_map = {0: 'BUY', 1: 'SELL', 2: 'HOLD'}
        
        results = []
        for i in range(len(preds_np)):
            results.append({
                'action': action_map[preds_np[i]],
                'confidence': float(probs_np[i].max()),
                'probabilities': {
                    'BUY': float(probs_np[i, 0]),
                    'SELL': float(probs_np[i, 1]),
                    'HOLD': float(probs_np[i, 2])
                }
            })
        
        return results[0] if len(results) == 1 else results
    
    def get_input_shape(self) -> Tuple[int, int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª"""
        return (self.sequence_length, self.input_dim)
    
    def _get_loss_function(self) -> nn.Module:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©"""
        return nn.CrossEntropyLoss()
    
    def get_attention_weights(self, x: np.ndarray) -> np.ndarray:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ù„ØªÙØ³ÙŠØ±
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            
        Returns:
            Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        """
        self.eval()
        
        if x.ndim == 2:
            x = x[np.newaxis, :, :]
        
        with torch.no_grad():
            x_tensor = torch.FloatTensor(x).to(self.device)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            selected, var_weights = self.variable_selection(x_tensor)
            
            # ØªØ´ÙÙŠØ± LSTM
            lstm_out, _ = self.lstm_encoder(selected)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            attention_output = lstm_out
            all_weights = []
            
            for i in range(self.num_encoder_layers):
                _, attn_weights = self.attention_layers[i](
                    attention_output, attention_output, attention_output
                )
                all_weights.append(attn_weights.cpu().numpy())
                attention_output = self.layer_norms[i](attention_output)
                attention_output = self.post_attention_grn[i](attention_output)
        
        return np.stack(all_weights, axis=0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = TFTModel(
        input_dim=50,
        hidden_dim=64,
        num_heads=4,
        num_encoder_layers=2,
        sequence_length=60,
        output_dim=3
    )
    
    # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    x = np.random.randn(4, 60, 50).astype(np.float32)
    
    # ØªÙ†Ø¨Ø¤
    result = model.predict(x)
    print(f"Predictions: {result}")
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print(f"\nModel info: {model.get_model_info()}")
