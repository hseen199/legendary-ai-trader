"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - LSTM with Attention
Ù†Ù…ÙˆØ°Ø¬ LSTM Ù…Ø¹ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from loguru import logger

from .base_model import BaseModel


class Attention(nn.Module):
    """Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡"""
    
    def __init__(self, hidden_dim: int, attention_dim: int):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        
        Args:
            hidden_dim: Ø­Ø¬Ù… Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
            attention_dim: Ø­Ø¬Ù… Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        """
        super().__init__()
        
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, attention_dim),
            nn.Tanh(),
            nn.Linear(attention_dim, 1)
        )
    
    def forward(self, lstm_output: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        
        Args:
            lstm_output: Ù…Ø®Ø±Ø¬Ø§Øª LSTM [batch, seq, hidden]
            
        Returns:
            (context_vector, attention_weights)
        """
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attention_scores = self.attention(lstm_output)  # [batch, seq, 1]
        attention_weights = F.softmax(attention_scores, dim=1)  # [batch, seq, 1]
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªØ¬Ù‡ Ø§Ù„Ø³ÙŠØ§Ù‚
        context_vector = torch.sum(attention_weights * lstm_output, dim=1)  # [batch, hidden]
        
        return context_vector, attention_weights.squeeze(-1)


class MultiHeadSelfAttention(nn.Module):
    """Ø§Ù†ØªØ¨Ø§Ù‡ Ø°Ø§ØªÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³"""
    
    def __init__(self, hidden_dim: int, num_heads: int = 4, dropout: float = 0.1):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        
        Args:
            hidden_dim: Ø­Ø¬Ù… Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
            num_heads: Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·
        """
        super().__init__()
        
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = self.head_dim ** -0.5
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch, seq, hidden]
            
        Returns:
            (output, attention_weights)
        """
        batch_size, seq_len, _ = x.size()
        
        # Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©
        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        context = torch.matmul(attention_weights, v)
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)
        
        output = self.out(context)
        
        # Ù…ØªÙˆØ³Ø· Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        avg_weights = attention_weights.mean(dim=1)
        
        return output, avg_weights


class LSTMAttentionModel(BaseModel):
    """
    Ù†Ù…ÙˆØ°Ø¬ LSTM Ù…Ø¹ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    
    ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ†:
    - LSTM Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
    - Ø§Ù†ØªØ¨Ø§Ù‡ Ø°Ø§ØªÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
    - Ø§Ù†ØªØ¨Ø§Ù‡ ØªØ³Ù„Ø³Ù„ÙŠ Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø©
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        num_layers: int = 2,
        num_heads: int = 4,
        dropout: float = 0.2,
        bidirectional: bool = True,
        sequence_length: int = 60,
        output_dim: int = 3,
        config: Optional[Dict] = None
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            input_dim: Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            hidden_dim: Ø­Ø¬Ù… Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
            num_layers: Ø¹Ø¯Ø¯ Ø·Ø¨Ù‚Ø§Øª LSTM
            num_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·
            bidirectional: Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            sequence_length: Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            output_dim: Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        """
        config = config or {}
        config.update({
            'input_dim': input_dim,
            'hidden_dim': hidden_dim,
            'num_layers': num_layers,
            'num_heads': num_heads,
            'dropout': dropout,
            'bidirectional': bidirectional,
            'sequence_length': sequence_length,
            'output_dim': output_dim
        })
        
        super().__init__("LSTM_Attention", config)
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.sequence_length = sequence_length
        self.output_dim = output_dim
        
        # Ø­Ø¬Ù… Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠ Ù…Ù† LSTM
        self.lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        self.input_projection = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # LSTM
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        # ØªØ·Ø¨ÙŠØ¹ Ø¨Ø¹Ø¯ LSTM
        self.lstm_norm = nn.LayerNorm(self.lstm_output_dim)
        
        # Ø§Ù†ØªØ¨Ø§Ù‡ Ø°Ø§ØªÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        self.self_attention = MultiHeadSelfAttention(
            self.lstm_output_dim, num_heads, dropout
        )
        self.attention_norm = nn.LayerNorm(self.lstm_output_dim)
        
        # Ø§Ù†ØªØ¨Ø§Ù‡ ØªØ³Ù„Ø³Ù„ÙŠ
        self.sequence_attention = Attention(self.lstm_output_dim, hidden_dim)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.classifier = nn.Sequential(
            nn.Linear(self.lstm_output_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout / 2),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
        
        logger.info(f"ğŸ§  LSTM Attention Model: {self.count_parameters():,} parameters")
    
    def _init_weights(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
                        # ØªØ¹ÙŠÙŠÙ† forget gate bias Ø¥Ù„Ù‰ 1
                        n = param.size(0)
                        param.data[n//4:n//2].fill_(1.0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch, sequence, features]
            
        Returns:
            Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª [batch, output_dim]
        """
        # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        x = self.input_projection(x)
        
        # LSTM
        lstm_out, (h_n, c_n) = self.lstm(x)
        lstm_out = self.lstm_norm(lstm_out)
        
        # Ø§Ù†ØªØ¨Ø§Ù‡ Ø°Ø§ØªÙŠ
        self_attn_out, _ = self.self_attention(lstm_out)
        lstm_out = self.attention_norm(lstm_out + self_attn_out)
        
        # Ø§Ù†ØªØ¨Ø§Ù‡ ØªØ³Ù„Ø³Ù„ÙŠ
        context, attention_weights = self.sequence_attention(lstm_out)
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ
        output = self.classifier(context)
        
        return output
    
    def forward_with_attention(
        self, 
        x: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ù…Ø¹ Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            
        Returns:
            (output, self_attention_weights, sequence_attention_weights)
        """
        # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        x = self.input_projection(x)
        
        # LSTM
        lstm_out, _ = self.lstm(x)
        lstm_out = self.lstm_norm(lstm_out)
        
        # Ø§Ù†ØªØ¨Ø§Ù‡ Ø°Ø§ØªÙŠ
        self_attn_out, self_attn_weights = self.self_attention(lstm_out)
        lstm_out = self.attention_norm(lstm_out + self_attn_out)
        
        # Ø§Ù†ØªØ¨Ø§Ù‡ ØªØ³Ù„Ø³Ù„ÙŠ
        context, seq_attn_weights = self.sequence_attention(lstm_out)
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ
        output = self.classifier(context)
        
        return output, self_attn_weights, seq_attn_weights
    
    def predict(self, x: np.ndarray) -> Dict[str, Any]:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        """
        self.eval()
        
        if x.ndim == 2:
            x = x[np.newaxis, :, :]
        
        with torch.no_grad():
            x_tensor = torch.FloatTensor(x).to(self.device)
            logits, self_attn, seq_attn = self.forward_with_attention(x_tensor)
            probs = F.softmax(logits, dim=-1)
            predictions = torch.argmax(probs, dim=-1)
        
        probs_np = probs.cpu().numpy()
        preds_np = predictions.cpu().numpy()
        seq_attn_np = seq_attn.cpu().numpy()
        
        action_map = {0: 'BUY', 1: 'SELL', 2: 'HOLD'}
        
        results = []
        for i in range(len(preds_np)):
            results.append({
                'action': action_map[preds_np[i]],
                'confidence': float(probs_np[i].max()),
                'probabilities': {
                    'BUY': float(probs_np[i, 0]),
                    'SELL': float(probs_np[i, 1]),
                    'HOLD': float(probs_np[i, 2])
                },
                'attention_weights': seq_attn_np[i].tolist()
            })
        
        return results[0] if len(results) == 1 else results
    
    def get_input_shape(self) -> Tuple[int, int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª"""
        return (self.sequence_length, self.input_dim)
    
    def _get_loss_function(self) -> nn.Module:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©"""
        return nn.CrossEntropyLoss()
    
    def get_feature_importance(self, x: np.ndarray) -> np.ndarray:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            
        Returns:
            Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
        """
        self.eval()
        
        if x.ndim == 2:
            x = x[np.newaxis, :, :]
        
        with torch.no_grad():
            x_tensor = torch.FloatTensor(x).to(self.device)
            _, _, seq_attn = self.forward_with_attention(x_tensor)
        
        # Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
        attention_weights = seq_attn.cpu().numpy()  # [batch, seq]
        
        # Ø§Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…ÙˆØ²ÙˆÙ† Ù„Ù„Ù…ÙŠØ²Ø§Øª
        importance = np.zeros((x.shape[0], x.shape[2]))
        for i in range(x.shape[0]):
            for j in range(x.shape[2]):
                importance[i, j] = np.sum(attention_weights[i] * x[i, :, j])
        
        # ØªØ·Ø¨ÙŠØ¹
        importance = np.abs(importance)
        importance = importance / (importance.sum(axis=1, keepdims=True) + 1e-10)
        
        return importance


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = LSTMAttentionModel(
        input_dim=50,
        hidden_dim=64,
        num_layers=2,
        num_heads=4,
        sequence_length=60,
        output_dim=3
    )
    
    # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    x = np.random.randn(4, 60, 50).astype(np.float32)
    
    # ØªÙ†Ø¨Ø¤
    result = model.predict(x)
    print(f"Predictions: {result}")
    
    # Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
    importance = model.get_feature_importance(x)
    print(f"\nFeature importance shape: {importance.shape}")
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print(f"\nModel info: {model.get_model_info()}")
