"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - Main Agent
Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ÙˆÙƒÙŠÙ„ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù„ØªØ¯Ø§ÙˆÙ„ ÙÙŠ Ø³ÙˆÙ‚ Ø§Ù„ÙƒØ±ÙŠØ¨ØªÙˆ
ÙŠÙÙƒØ±ØŒ ÙŠØ¨ØªÙƒØ±ØŒ ÙŠØªØ¹Ù„Ù…ØŒ ÙˆÙŠØªØ·ÙˆØ± Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„

Author: Legendary Agent Team
Version: 1.0.0
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from loguru import logger

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±
ROOT_DIR = Path(__file__).parent
sys.path.insert(0, str(ROOT_DIR))

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
from types import MarketState, TradingSignal, AgentDecision, AgentConfig
from layers.perception import PerceptionLayer
from layers.understanding import UnderstandingLayer
from layers.planning import PlanningLayer
from layers.decision import DecisionLayer
from layers.protection import ProtectionLayer
from layers.evolution import EvolutionLayer
from mind.creative_mind import CreativeMind
from memory.memory_system import MemorySystem, MemoryType
from protection.circuit_breaker import CircuitBreaker
from protection.anomaly_detector import AnomalyDetector
from models.ensemble import EnsembleModel


@dataclass
class AgentState:
    """Ø­Ø§Ù„Ø© Ø§Ù„ÙˆÙƒÙŠÙ„"""
    is_active: bool = True
    mode: str = "balanced"  # aggressive, balanced, conservative
    total_decisions: int = 0
    successful_decisions: int = 0
    current_positions: Dict[str, Any] = field(default_factory=dict)
    daily_pnl: float = 0.0
    weekly_pnl: float = 0.0
    last_update: datetime = field(default_factory=datetime.now)


class LegendaryAgent:
    """
    Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ
    
    ÙˆÙƒÙŠÙ„ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªÙƒØ§Ù…Ù„ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ†:
    - 6 Ø·Ø¨Ù‚Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø¥Ø¯Ø±Ø§ÙƒØŒ ÙÙ‡Ù…ØŒ ØªØ®Ø·ÙŠØ·ØŒ Ù‚Ø±Ø§Ø±ØŒ Ø­Ù…Ø§ÙŠØ©ØŒ ØªØ·ÙˆØ±)
    - Ø¹Ù‚Ù„ Ù…Ø¨Ø¯Ø¹ (ØªÙÙƒÙŠØ±ØŒ Ø§Ø¨ØªÙƒØ§Ø±ØŒ Ø­ÙˆØ§Ø± Ø¯Ø§Ø®Ù„ÙŠ)
    - Ù†Ø¸Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
    - Ø£Ù†Ø¸Ù…Ø© Ø­Ù…Ø§ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©
    - Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù…ØªØ¹Ø¯Ø¯Ø©
    """
    
    def __init__(
        self,
        config: Union[Dict, AgentConfig] = None,
        model_path: str = None,
        data_dir: str = None
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ
        
        Args:
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙˆÙƒÙŠÙ„
            model_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©
            data_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        if isinstance(config, dict):
            self.config = AgentConfig(**config) if config else AgentConfig()
        else:
            self.config = config or AgentConfig()
        
        self.model_path = Path(model_path) if model_path else ROOT_DIR / 'models' / 'trained'
        self.data_dir = Path(data_dir) if data_dir else ROOT_DIR / 'data'
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø§Ù„Ø©
        self.state = AgentState(mode=self.config.mode)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self._initialize_components()
        
        logger.info(f"ğŸ¦ LegendaryAgent initialized in {self.config.mode} mode on {self.device}")
    
    def _initialize_components(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
        config_dict = {
            'stop_loss': self.config.stop_loss,
            'take_profit': self.config.take_profit,
            'max_position_size': self.config.max_position_size,
            'min_position_size': self.config.min_position_size,
            'max_daily_loss': self.config.max_daily_loss,
            'max_weekly_loss': self.config.max_weekly_loss,
            'portfolio_heat_limit': self.config.portfolio_heat_limit
        }
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø³Øª
        logger.info("  Initializing 6 layers...")
        self.perception = PerceptionLayer(config_dict)
        self.understanding = UnderstandingLayer(config_dict)
        self.planning = PlanningLayer(config_dict)
        self.decision = DecisionLayer(config_dict)
        self.protection = ProtectionLayer(config_dict)
        self.evolution = EvolutionLayer(config_dict, str(self.data_dir / 'evolution'))
        
        # Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¨Ø¯Ø¹
        logger.info("  Initializing Creative Mind...")
        self.mind = CreativeMind(config_dict)
        
        # Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        logger.info("  Initializing Memory System...")
        self.memory = MemorySystem(config_dict, str(self.data_dir / 'memory'))
        
        # Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø­Ù…Ø§ÙŠØ©
        logger.info("  Initializing Protection Systems...")
        self.circuit_breaker = CircuitBreaker(config_dict)
        self.anomaly_detector = AnomalyDetector(config_dict)
        
        # Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        logger.info("  Loading ML Model...")
        self.model = self._load_model()
    
    def _load_model(self) -> Optional[EnsembleModel]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
        try:
            model_file = self.model_path / 'ensemble_model.pt'
            if model_file.exists():
                model = EnsembleModel(
                    num_features=self.config.num_features,
                    sequence_length=self.config.sequence_length,
                    hidden_dim=128
                )
                model.load_state_dict(torch.load(model_file, map_location=self.device))
                model.to(self.device)
                model.eval()
                logger.info(f"  âœ… Model loaded from {model_file}")
                return model
            else:
                logger.warning(f"  âš ï¸ Model file not found: {model_file}")
                return None
        except Exception as e:
            logger.error(f"  âŒ Failed to load model: {e}")
            return None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MAIN DECISION PIPELINE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def decide(
        self,
        symbol: str,
        market_data: Dict[str, Any],
        portfolio: Dict[str, Any] = None
    ) -> AgentDecision:
        """
        Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± ØªØ¯Ø§ÙˆÙ„
        
        Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ Ù…Ù† Ù†Ø¸Ø§Ù…Ùƒ
        
        Args:
            symbol: Ø±Ù…Ø² Ø§Ù„Ø¹Ù…Ù„Ø© (Ù…Ø«Ù„ BTCUSDT)
            market_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ (OHLCV, features, etc.)
            portfolio: Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­ÙØ¸Ø©
            
        Returns:
            Ù‚Ø±Ø§Ø± Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù…Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„
        """
        start_time = datetime.now()
        self.state.total_decisions += 1
        
        try:
            # 1. ÙØ­Øµ Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø¯Ø§Ø¦Ø±Ø©
            breaker_status = self.circuit_breaker.check(
                daily_pnl=self.state.daily_pnl,
                weekly_pnl=self.state.weekly_pnl,
                consecutive_losses=self._count_consecutive_losses(),
                price_change_5m=market_data.get('price_change_5m', 0),
                volatility=market_data.get('volatility', 0)
            )
            
            if not breaker_status.is_trading_allowed:
                return self._create_hold_decision(
                    symbol,
                    f"Circuit breaker active: {breaker_status.last_trip_reason}",
                    confidence=0.0
                )
            
            # 2. ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
            anomaly_result = self.anomaly_detector.detect(
                symbol=symbol,
                current_price=market_data.get('close', 0),
                features=market_data.get('features', {}),
                orderbook=market_data.get('orderbook')
            )
            
            if anomaly_result.risk_score > 0.7:
                return self._create_hold_decision(
                    symbol,
                    f"High anomaly risk: {anomaly_result.recommendations[0] if anomaly_result.recommendations else 'Unknown'}",
                    confidence=0.0
                )
            
            # 3. Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø±Ø§Ùƒ
            perception_state = self.perception.perceive(
                symbol=symbol,
                ohlcv=market_data.get('ohlcv', []),
                features=market_data.get('features', {}),
                orderbook=market_data.get('orderbook')
            )
            
            # 4. Ø·Ø¨Ù‚Ø© Ø§Ù„ÙÙ‡Ù…
            understanding_state = self.understanding.understand(perception_state)
            
            # 5. Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ®Ø·ÙŠØ·
            planning_state = self.planning.plan(
                understanding=understanding_state,
                portfolio=portfolio or {},
                open_positions=self.state.current_positions
            )
            
            # 6. Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¨Ø¯Ø¹ - Ø§Ù„ØªÙÙƒÙŠØ±
            creative_output = self.mind.think(
                symbol=symbol,
                market_state=understanding_state,
                memory_context=self.memory.build_context(symbol)
            )
            
            # 7. Ø·Ø¨Ù‚Ø© Ø§Ù„Ø­Ù…Ø§ÙŠØ©
            protection_status = self.protection.check(
                symbol=symbol,
                proposed_action=creative_output.get('suggested_action', 'HOLD'),
                position_size=planning_state.suggested_position_size,
                portfolio=portfolio or {},
                market_state=understanding_state
            )
            
            if not protection_status.is_safe:
                return self._create_hold_decision(
                    symbol,
                    f"Protection blocked: {protection_status.alerts[0].message if protection_status.alerts else 'Risk too high'}",
                    confidence=0.3
                )
            
            # 8. ØªÙ†Ø¨Ø¤ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±)
            model_prediction = None
            if self.model and 'features_tensor' in market_data:
                model_prediction = self._get_model_prediction(market_data['features_tensor'])
            
            # 9. Ø·Ø¨Ù‚Ø© Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            decision = self.decision.decide(
                symbol=symbol,
                understanding=understanding_state,
                planning=planning_state,
                creative_output=creative_output,
                model_prediction=model_prediction,
                protection_status=protection_status
            )
            
            # 10. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.memory.remember(
                {
                    'symbol': symbol,
                    'decision': decision.action,
                    'confidence': decision.confidence,
                    'regime': understanding_state.regime.value if hasattr(understanding_state, 'regime') else 'UNKNOWN',
                    'timestamp': datetime.now().isoformat()
                },
                memory_type=MemoryType.SHORT_TERM,
                importance=decision.confidence,
                tags=[symbol, decision.action]
            )
            
            # 11. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return AgentDecision(
                symbol=symbol,
                action=decision.action,
                confidence=decision.confidence,
                position_size_percent=decision.position_size,
                entry_price=market_data.get('close', 0),
                stop_loss=self._calculate_stop_loss(market_data.get('close', 0), decision.action),
                take_profit_levels=self._calculate_take_profits(market_data.get('close', 0), decision.action),
                reasoning=decision.reasoning,
                risk_score=protection_status.risk_score,
                market_regime=understanding_state.regime.value if hasattr(understanding_state, 'regime') else 'UNKNOWN',
                creative_insight=creative_output.get('insight', ''),
                processing_time_ms=processing_time,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"âŒ Decision error for {symbol}: {e}")
            return self._create_hold_decision(symbol, f"Error: {str(e)}", confidence=0.0)
    
    def _get_model_prediction(self, features_tensor: torch.Tensor) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            with torch.no_grad():
                features_tensor = features_tensor.to(self.device)
                if features_tensor.dim() == 2:
                    features_tensor = features_tensor.unsqueeze(0)
                
                output = self.model(features_tensor)
                
                return {
                    'prediction': output['final_prediction'].cpu().numpy()[0],
                    'regime_probs': output['regime_probs'].cpu().numpy()[0] if 'regime_probs' in output else None,
                    'confidence': output.get('confidence', 0.5)
                }
        except Exception as e:
            logger.error(f"Model prediction error: {e}")
            return None
    
    def _create_hold_decision(
        self,
        symbol: str,
        reason: str,
        confidence: float
    ) -> AgentDecision:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø±Ø§Ø± Ø§Ù†ØªØ¸Ø§Ø±"""
        return AgentDecision(
            symbol=symbol,
            action='HOLD',
            confidence=confidence,
            position_size_percent=0,
            entry_price=0,
            stop_loss=0,
            take_profit_levels=[],
            reasoning=reason,
            risk_score=1.0,
            market_regime='UNKNOWN',
            creative_insight='',
            processing_time_ms=0,
            timestamp=datetime.now()
        )
    
    def _calculate_stop_loss(self, price: float, action: str) -> float:
        """Ø­Ø³Ø§Ø¨ ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©"""
        if action == 'BUY':
            return price * (1 - self.config.stop_loss / 100)
        elif action == 'SELL':
            return price * (1 + self.config.stop_loss / 100)
        return 0
    
    def _calculate_take_profits(self, price: float, action: str) -> List[float]:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø¬Ù†ÙŠ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­"""
        if action not in ['BUY', 'SELL']:
            return []
        
        levels = []
        for tp in self.config.take_profit:
            if action == 'BUY':
                levels.append(price * (1 + tp / 100))
            else:
                levels.append(price * (1 - tp / 100))
        
        return levels
    
    def _count_consecutive_losses(self) -> int:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø¦Ø± Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ©"""
        # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ø¨Ø§Ù„Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø°Ø§ÙƒØ±Ø©
        return 0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # LEARNING & EVOLUTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def learn_from_trade(
        self,
        symbol: str,
        action: str,
        entry_price: float,
        exit_price: float,
        holding_time: int,
        market_regime: str,
        features_at_entry: Dict[str, float],
        features_at_exit: Dict[str, float],
        decision_confidence: float
    ) -> None:
        """
        Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† ØµÙÙ‚Ø©
        
        Ø§Ø³ØªØ¯Ø¹Ù Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø¹Ø¯ Ø¥ØºÙ„Ø§Ù‚ ÙƒÙ„ ØµÙÙ‚Ø©
        
        Args:
            symbol: Ø±Ù…Ø² Ø§Ù„Ø¹Ù…Ù„Ø©
            action: Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡
            entry_price: Ø³Ø¹Ø± Ø§Ù„Ø¯Ø®ÙˆÙ„
            exit_price: Ø³Ø¹Ø± Ø§Ù„Ø®Ø±ÙˆØ¬
            holding_time: ÙˆÙ‚Øª Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚
            market_regime: Ù†Ø¸Ø§Ù… Ø§Ù„Ø³ÙˆÙ‚
            features_at_entry: Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø¯Ø®ÙˆÙ„
            features_at_exit: Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø±ÙˆØ¬
            decision_confidence: Ø«Ù‚Ø© Ø§Ù„Ù‚Ø±Ø§Ø±
        """
        # Ø§Ù„ØªØ¹Ù„Ù… ÙÙŠ Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ·ÙˆØ±
        lesson = self.evolution.learn_from_trade(
            symbol=symbol,
            action=action,
            entry_price=entry_price,
            exit_price=exit_price,
            holding_time=holding_time,
            market_regime=market_regime,
            features_at_entry=features_at_entry,
            features_at_exit=features_at_exit,
            decision_confidence=decision_confidence
        )
        
        # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        pnl = (exit_price - entry_price) / entry_price * 100 if action == 'BUY' else (entry_price - exit_price) / entry_price * 100
        outcome = 'WIN' if pnl > 0 else 'LOSS'
        
        self.memory.record_trade(
            symbol=symbol,
            action=action,
            entry_price=entry_price,
            exit_price=exit_price,
            pnl=pnl,
            context={'regime': market_regime, 'confidence': decision_confidence},
            outcome=outcome
        )
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.state.daily_pnl += pnl
        self.state.weekly_pnl += pnl
        
        if outcome == 'WIN':
            self.state.successful_decisions += 1
        
        # ØªØ«Ø¨ÙŠØª Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        self.memory.consolidate()
        
        logger.info(f"ğŸ“š Learned from {symbol} trade: {outcome} ({pnl:+.2f}%)")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STATUS & MANAGEMENT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„ÙˆÙƒÙŠÙ„"""
        return {
            'is_active': self.state.is_active,
            'mode': self.state.mode,
            'total_decisions': self.state.total_decisions,
            'successful_decisions': self.state.successful_decisions,
            'success_rate': (
                self.state.successful_decisions / self.state.total_decisions
                if self.state.total_decisions > 0 else 0
            ),
            'daily_pnl': f"{self.state.daily_pnl:+.2f}%",
            'weekly_pnl': f"{self.state.weekly_pnl:+.2f}%",
            'open_positions': len(self.state.current_positions),
            'circuit_breaker': self.circuit_breaker.get_status().state.value,
            'memory_status': self.memory.get_status(),
            'evolution_status': self.evolution.get_status(),
            'last_update': self.state.last_update.isoformat()
        }
    
    def set_mode(self, mode: str) -> None:
        """ØªØºÙŠÙŠØ± ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø§ÙˆÙ„"""
        if mode in ['aggressive', 'balanced', 'conservative']:
            self.state.mode = mode
            self.config.mode = mode
            logger.info(f"ğŸ”„ Mode changed to: {mode}")
    
    def pause(self) -> None:
        """Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¤Ù‚Øª"""
        self.state.is_active = False
        logger.info("â¸ï¸ Agent paused")
    
    def resume(self) -> None:
        """Ø§Ø³ØªØ¦Ù†Ø§Ù"""
        self.state.is_active = True
        logger.info("â–¶ï¸ Agent resumed")
    
    def reset_daily_stats(self) -> None:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©"""
        self.state.daily_pnl = 0
        logger.info("ğŸ“Š Daily stats reset")
    
    def reset_weekly_stats(self) -> None:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠØ©"""
        self.state.weekly_pnl = 0
        logger.info("ğŸ“Š Weekly stats reset")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆÙƒÙŠÙ„
    print("ğŸ¦ Testing Legendary Agent...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„
    agent = LegendaryAgent()
    
    # Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    market_data = {
        'close': 50000,
        'high': 50500,
        'low': 49500,
        'volume': 1000000,
        'price_change_5m': 0.5,
        'volatility': 2.0,
        'ohlcv': [[50000, 50500, 49500, 50200, 1000000]],
        'features': {
            'rsi_14': 55,
            'macd': 100,
            'macd_signal': 80,
            'bb_upper': 51000,
            'bb_lower': 49000,
            'atr_14': 500,
            'volume_sma_20': 900000
        }
    }
    
    portfolio = {
        'balance': 10000,
        'available': 8000,
        'positions': {}
    }
    
    # Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±
    decision = agent.decide('BTCUSDT', market_data, portfolio)
    
    print(f"\nğŸ“‹ Decision:")
    print(f"   Symbol: {decision.symbol}")
    print(f"   Action: {decision.action}")
    print(f"   Confidence: {decision.confidence:.1%}")
    print(f"   Position Size: {decision.position_size_percent:.1f}%")
    print(f"   Stop Loss: ${decision.stop_loss:,.2f}")
    print(f"   Take Profits: {[f'${tp:,.2f}' for tp in decision.take_profit_levels]}")
    print(f"   Reasoning: {decision.reasoning}")
    print(f"   Risk Score: {decision.risk_score:.2f}")
    print(f"   Processing Time: {decision.processing_time_ms:.1f}ms")
    
    print(f"\nğŸ“Š Agent Status:")
    status = agent.get_status()
    for key, value in status.items():
        print(f"   {key}: {value}")
