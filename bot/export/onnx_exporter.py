"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LEGENDARY AGENT - ONNX Exporter
Ù…ØµØ¯Ø± ONNX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from loguru import logger

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø±
sys.path.insert(0, str(Path(__file__).parent.parent))

from models.ensemble import EnsembleModel
from models.tft_model import TFTModel
from models.lstm_attention import LSTMAttentionModel


class SimplifiedModel(nn.Module):
    """
    Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø³Ø· Ù„Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ONNX
    
    ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† TFT Ùˆ LSTM ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØµØ¯ÙŠØ±
    """
    
    def __init__(
        self,
        num_features: int = 50,
        sequence_length: int = 60,
        hidden_dim: int = 128
    ):
        super().__init__()
        
        self.num_features = num_features
        self.sequence_length = sequence_length
        self.hidden_dim = hidden_dim
        
        # LSTM encoder
        self.lstm = nn.LSTM(
            input_size=num_features,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.2,
            bidirectional=True
        )
        
        # Attention
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim * 2,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        # Feature extraction
        self.feature_extractor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # Output heads
        self.prediction_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Tanh()  # Output between -1 and 1
        )
        
        self.confidence_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Output between 0 and 1
        )
        
        self.regime_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Linear(32, 7),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass
        
        Args:
            x: Input tensor [batch, seq_len, features]
            
        Returns:
            prediction: Price direction prediction [-1, 1]
            confidence: Confidence score [0, 1]
            regime_probs: Market regime probabilities [7]
        """
        # LSTM encoding
        lstm_out, _ = self.lstm(x)
        
        # Self-attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Global average pooling
        pooled = attn_out.mean(dim=1)
        
        # Feature extraction
        features = self.feature_extractor(pooled)
        
        # Output heads
        prediction = self.prediction_head(features)
        confidence = self.confidence_head(features)
        regime_probs = self.regime_head(features)
        
        return prediction, confidence, regime_probs


class ONNXExporter:
    """
    Ù…ØµØ¯Ø± ONNX
    
    ÙŠØµØ¯Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ ØµÙŠØºØ© ONNX Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ TypeScript
    """
    
    def __init__(self, output_dir: str = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØµØ¯Ø±
        
        Args:
            output_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        """
        self.output_dir = Path(output_dir) if output_dir else Path('/tmp/legendary_agent/export')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“¦ ONNXExporter initialized, output: {self.output_dir}")
    
    def export_simplified(
        self,
        num_features: int = 50,
        sequence_length: int = 60,
        hidden_dim: int = 128,
        output_name: str = 'legendary_agent.onnx'
    ) -> str:
        """
        ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¨Ø³Ø·
        
        Args:
            num_features: Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            sequence_length: Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            hidden_dim: Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
            output_name: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            
        Returns:
            Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø±
        """
        logger.info("ğŸ”„ Creating simplified model for export...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model = SimplifiedModel(
            num_features=num_features,
            sequence_length=sequence_length,
            hidden_dim=hidden_dim
        )
        model.eval()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø®Ù„ ÙˆÙ‡Ù…ÙŠ
        dummy_input = torch.randn(1, sequence_length, num_features)
        
        # Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
        output_path = self.output_dir / output_name
        
        logger.info(f"ğŸ“¦ Exporting to {output_path}...")
        
        # Ø§Ù„ØªØµØ¯ÙŠØ±
        torch.onnx.export(
            model,
            dummy_input,
            str(output_path),
            input_names=['features'],
            output_names=['prediction', 'confidence', 'regime_probs'],
            dynamic_axes={
                'features': {0: 'batch_size'},
                'prediction': {0: 'batch_size'},
                'confidence': {0: 'batch_size'},
                'regime_probs': {0: 'batch_size'}
            },
            opset_version=11,
            do_constant_folding=True,
            export_params=True
        )
        
        logger.info(f"âœ… Model exported successfully: {output_path}")
        logger.info(f"   Size: {output_path.stat().st_size / 1024:.1f} KB")
        
        return str(output_path)
    
    def export_ensemble(
        self,
        ensemble_model: EnsembleModel,
        output_name: str = 'legendary_ensemble.onnx'
    ) -> str:
        """
        ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ù…Ø¬
        
        Args:
            ensemble_model: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
            output_name: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            
        Returns:
            Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø±
        """
        logger.info("ğŸ”„ Exporting ensemble model...")
        
        ensemble_model.eval()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø®Ù„ ÙˆÙ‡Ù…ÙŠ
        dummy_input = torch.randn(1, 60, 50)
        
        # Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
        output_path = self.output_dir / output_name
        
        try:
            torch.onnx.export(
                ensemble_model,
                dummy_input,
                str(output_path),
                input_names=['features'],
                output_names=['output'],
                dynamic_axes={
                    'features': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                },
                opset_version=11
            )
            
            logger.info(f"âœ… Ensemble exported: {output_path}")
            return str(output_path)
            
        except Exception as e:
            logger.error(f"âŒ Export failed: {e}")
            logger.info("   Falling back to simplified model...")
            return self.export_simplified(output_name=output_name)
    
    def verify_export(self, onnx_path: str) -> bool:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªØµØ¯ÙŠØ±
        
        Args:
            onnx_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù ONNX
            
        Returns:
            True Ø¥Ø°Ø§ ÙƒØ§Ù† ØµØ§Ù„Ø­Ø§Ù‹
        """
        try:
            import onnx
            
            model = onnx.load(onnx_path)
            onnx.checker.check_model(model)
            
            logger.info(f"âœ… ONNX model verification passed")
            
            # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            logger.info(f"   Inputs: {[i.name for i in model.graph.input]}")
            logger.info(f"   Outputs: {[o.name for o in model.graph.output]}")
            
            return True
            
        except ImportError:
            logger.warning("âš ï¸ onnx package not installed, skipping verification")
            return True
        except Exception as e:
            logger.error(f"âŒ Verification failed: {e}")
            return False
    
    def test_inference(self, onnx_path: str) -> Dict[str, Any]:
        """
        Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        
        Args:
            onnx_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù ONNX
            
        Returns:
            Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        """
        try:
            import onnxruntime as ort
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©
            session = ort.InferenceSession(onnx_path)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø®Ù„ ÙˆÙ‡Ù…ÙŠ
            dummy_input = np.random.randn(1, 60, 50).astype(np.float32)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
            outputs = session.run(None, {'features': dummy_input})
            
            result = {
                'success': True,
                'num_outputs': len(outputs),
                'output_shapes': [o.shape for o in outputs],
                'sample_prediction': float(outputs[0][0][0]) if len(outputs) > 0 else None,
                'sample_confidence': float(outputs[1][0][0]) if len(outputs) > 1 else None
            }
            
            logger.info(f"âœ… Inference test passed")
            logger.info(f"   Prediction: {result['sample_prediction']:.4f}")
            logger.info(f"   Confidence: {result['sample_confidence']:.4f}")
            
            return result
            
        except ImportError:
            logger.warning("âš ï¸ onnxruntime not installed, skipping inference test")
            return {'success': False, 'error': 'onnxruntime not installed'}
        except Exception as e:
            logger.error(f"âŒ Inference test failed: {e}")
            return {'success': False, 'error': str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    exporter = ONNXExporter(output_dir='/home/ubuntu/legendary_agent/models/trained')
    
    # ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¨Ø³Ø·
    onnx_path = exporter.export_simplified(
        num_features=50,
        sequence_length=60,
        hidden_dim=128,
        output_name='legendary_agent.onnx'
    )
    
    # Ø§Ù„ØªØ­Ù‚Ù‚
    exporter.verify_export(onnx_path)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
    result = exporter.test_inference(onnx_path)
    print(f"\nInference result: {result}")
